{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of Copy of Copy of Practical_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forde1909/MLDIoT_Practical1/blob/master/Copy_of_Copy_of_Copy_of_Practical_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrvM4bz0Fr5F"
      },
      "source": [
        "#Programming assignment 1: introductory tour and decision trees\n",
        "In this assignment, you will take a quick guided tour of the scikit-learn library, one of the most widely used machine learning libraries in Python. We will particularly focus on decision tree learning for classification and regression.\n",
        "\n",
        "\n",
        "There are three tasks of the assignment, where the main focus is on using scikit-learn for training and evaluating machine learning models. \n",
        "\n",
        "Save this notebook in your own google drive/github account, complete your code, plots, and comments and save as a .ipynb file. Name the file as \"Assignment1_your_name.ipynb\" and submit to the assignmet for this practical on blackboard.\n",
        "\n",
        "Deadline: 06-11-2020\n",
        "\n",
        "Didactic purpose of this assignment:\n",
        "\n",
        "*   getting a feel for the workflow of machine learning in Python;\n",
        "*   understanding machine learning algorithms for classification and regression;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5InA2wWFk0n"
      },
      "source": [
        "# Task 1: A classification example: fetal heart condition diagnosis\n",
        "The UCI Machine Learning Repository contains several datasets that can be used to investigate different machine learning algorithms. In this exercise, we'll use a dataset of fetal heart diagnosis. The dataset contains measurements from about 2,600 fetuses. This is a classification task, where our task is to predict a diagnosis type following the FIGO Intrapartum Fetal Monitoring Guidelines: normal, suspicious, or pathological."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUNxdOrFk0s"
      },
      "source": [
        "# Step 1. Reading the data\n",
        "\n",
        "This file contains the data that we will use. This file contains the same data as in the public distribution, except that we converted from Excel to CSV. Download the file and save it in a working directory.\n",
        "\n",
        "Open your favorite editor or a Jupyter notebook. To read the CSV file, it is probably easiest to use the Pandas library. Here is a code snippet that carries out the relevant steps:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ziCVTYBFk0x",
        "outputId": "fa8b9083-a964-41ea-b6ca-253b3e010e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "  \n",
        "# Read the CSV file.\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/niallomahony93/MLDIoT_Practical1/master/Assignment1/CTG.csv\", skiprows=1)\n",
        "\n",
        "# Select the relevant numerical columns.\n",
        "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
        "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
        "                 'Median', 'Variance', 'Tendency', 'NSP']\n",
        "data = data[selected_cols].dropna()\n",
        "\n",
        "# Shuffle the dataset.\n",
        "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
        "\n",
        "# Split into input part X and output part Y.\n",
        "X = data_shuffled.drop('NSP', axis=1)\n",
        "\n",
        "# Map the diagnosis code to a human-readable label.\n",
        "def to_label(y):\n",
        "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n",
        "\n",
        "Y = data_shuffled['NSP'].apply(to_label)\n",
        "\n",
        "\n",
        "# Partition the data into training and test sets.\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "Y.head()\n",
        "X.head()\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LB</th>\n",
              "      <th>AC</th>\n",
              "      <th>FM</th>\n",
              "      <th>UC</th>\n",
              "      <th>DL</th>\n",
              "      <th>DS</th>\n",
              "      <th>DP</th>\n",
              "      <th>ASTV</th>\n",
              "      <th>MSTV</th>\n",
              "      <th>ALTV</th>\n",
              "      <th>MLTV</th>\n",
              "      <th>Width</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>Nmax</th>\n",
              "      <th>Nzeros</th>\n",
              "      <th>Mode</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Median</th>\n",
              "      <th>Variance</th>\n",
              "      <th>Tendency</th>\n",
              "      <th>NSP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>43.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>64.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>132.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>133.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>134.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>132.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.9</td>\n",
              "      <td>117.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      LB   AC   FM   UC   DL  ...   Mean  Median  Variance  Tendency  NSP\n",
              "0  120.0  0.0  0.0  0.0  0.0  ...  137.0   121.0      73.0       1.0  2.0\n",
              "1  132.0  4.0  0.0  4.0  2.0  ...  136.0   140.0      12.0       0.0  1.0\n",
              "2  133.0  2.0  0.0  5.0  2.0  ...  135.0   138.0      13.0       0.0  1.0\n",
              "3  134.0  2.0  0.0  6.0  2.0  ...  134.0   137.0      13.0       1.0  1.0\n",
              "4  132.0  4.0  0.0  5.0  0.0  ...  136.0   138.0      11.0       1.0  1.0\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1kALXYvFk04"
      },
      "source": [
        "# Step 2. Training the baseline classifier\n",
        "\n",
        "We can now start to investigate different classifiers.\n",
        "The DummyClassifier is a simple classifier that does not make use of the features: it just returns the most common label in the training set, in this case Spondylolisthesis. The purpose of using such a stupid classifier is as a baseline: a simple classifier that we can try before we move on to more complex classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EixHbH8mFk07"
      },
      "source": [
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier(strategy='most_frequent')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fidwG14tIfio"
      },
      "source": [
        "To get an idea of how well our simple classifier works, we carry out a cross-validation over the training set and compute the classification accuracy on each fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIAN2Ji8IoGa",
        "outputId": "59cf3f70-9e53-447e-8810-d49fb87f19ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cross_val_score(clf, Xtrain, Ytrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.78235294, 0.78235294, 0.77941176, 0.77941176, 0.77941176])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dCfZ9RPOHD"
      },
      "source": [
        "The result is a NumPy array that contains the accuracies on the different folds in the cross-validation. Get the mean accuracy with the .mean() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsspBBuqJCus",
        "outputId": "3f5d150d-0db8-4ec0-b5d1-5216739d4efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7805882352941176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TjHhnt-Fk1E"
      },
      "source": [
        "## Step 3. Trying out some different classifiers\n",
        "Replace the DummyClassifier with some more meaningful classifier and run the cross-validation again. Try out a few classifiers and see how much you can improve the cross-validation accuracy. Remember, the accuracy is defined as the proportion of correctly classified instances, and we want this value to be high.\n",
        "\n",
        "\n",
        "Here are some possible options:\n",
        "\n",
        "Tree-based classifiers:\n",
        "\n",
        "sklearn.tree.DecisionTreeClassifier\n",
        "\n",
        "sklearn.ensemble.RandomForestClassifier\n",
        "\n",
        "sklearn.ensemble.GradientBoostingClassifier\n",
        "\n",
        "\n",
        "Linear classifiers:\n",
        "\n",
        "sklearn.linear_model.Perceptron\n",
        "\n",
        "sklearn.linear_model.LogisticRegression\n",
        "\n",
        "sklearn.svm.LinearSVC\n",
        "\n",
        "\n",
        "Neural network classifier (will take longer time to train):\n",
        "\n",
        "sklearn.neural_network.MLPClassifier\n",
        "\n",
        "You may also try to tune the hyperparameters of the various classifiers to improve the performance. For instance, the decision tree classifier has a parameter that sets the maximum depth, and in the neural network classifier you can control the number of layers and the number of neurons in each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUDTHf2oFk1F"
      },
      "source": [
        "# The Tree-based classifier using DecisionTreeClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rdH36_yFk1G",
        "outputId": "aa166c63-483d-46e6-cf61-7ceb2c538593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9194117647058823"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVqKHLDJFk1J"
      },
      "source": [
        "# The Tree-based classifier using RandomForestClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Yg75XWFk1M",
        "outputId": "ae617a18-92f7-4598-8936-758552425758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "clf = ensemble.RandomForestClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9411764705882353"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61PWJprlFk1V"
      },
      "source": [
        "# The Tree-based classifier using GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I00Mq5jFk1Z",
        "outputId": "02531871-2895-45b4-aa5b-fc70d24db4d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "clf = ensemble.GradientBoostingClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9488235294117647"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStwsBJYFk1k"
      },
      "source": [
        "# Linear classifier using Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtFidw3CFk1l",
        "outputId": "c09d6358-0b3b-47cd-8329-013e5cfba875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Perceptron()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.825294117647059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je04C9W5Fk1p"
      },
      "source": [
        "# Linear classifier using LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK8zzfkAFk1p",
        "outputId": "6a43c9f2-6483-4965-8039-3b8f025d3f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.LogisticRegression()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8735294117647058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSqJ2KUPFk10"
      },
      "source": [
        "# Linear classifier using LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5LFSpADFk12",
        "outputId": "6932c005-81b7-4455-b352-e7d0ffb5ec1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.836470588235294"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQmRypeP6lo"
      },
      "source": [
        " Neural Network Using Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcOMM3NOFk17",
        "outputId": "d7291043-3fbc-4563-c3c4-4d36c4a6a320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import neural_network\n",
        "clf = neural_network.MLPClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8800000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8z-1HaOFk2G"
      },
      "source": [
        "# Step 4. Final evaluation\n",
        "When you have found a classifier that gives a high accuracy in the cross-validation evaluation, train it on the whole training set and evaluate it on the held-out test set. Please include a description of the classifier you selected and report its accuracy below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyT5Yqs5Fk2G"
      },
      "source": [
        "According to step 3, the best results were made using GradientBoostingClassifier and the accuracy is about 0.9488235294117647."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk0fh09UFk2H",
        "outputId": "53d31de7-e17c-4f1d-99f7-5f9632fe6a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "  \n",
        "clf.fit(Xtrain, Ytrain)\n",
        "Yguess = clf.predict(Xtest)\n",
        "print(accuracy_score(Ytest, Yguess))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9272300469483568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQnMHsLSFk2L"
      },
      "source": [
        "# Task 2: Decision trees for classification\n",
        "Import the code from Lecture1.ipynb and use the defined class TreeClassifier as your classifier in an experiment similar to those in Task 1. Tune the hyperparameter max_depth to get the best cross-validation performance, and then evaluate the classifier on the test set.\n",
        "\n",
        "Please report below what value of max_depth you selected and what accuracy you got.\n",
        "\n",
        "For illustration, let's also draw a tree. Set max_depth to a reasonable small value, and then call draw_tree to visualize the learned decision tree. Include this tree in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ssyp1b3Fk26"
      },
      "source": [
        "The accuracy of the classifier in the test set is ____. The accuracy is largely increased at the max_depth _______. The learned decision tree is visualized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiC2IwGhFk27",
        "outputId": "ae9b6379-e081-4683-afb4-1d614b616233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "clf = ensemble.RandomForestClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9394117647058824"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_48wtXTE9T-"
      },
      "source": [
        "import numpy as np\n",
        "class DecisionTreeLeaf:\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
        "    def predict(self, x):\n",
        "        return self.value\n",
        "\n",
        "    # Utility function to draw a tree visually using graphviz.\n",
        "    def draw_tree(self, graph, node_counter, names):\n",
        "        node_id = str(node_counter)\n",
        "        graph.node(node_id, str(self.value), style='filled')\n",
        "        return node_counter+1, node_id\n",
        "        \n",
        "    def __eq__(self, other):\n",
        "        if isinstance(other, DecisionTreeLeaf):\n",
        "            return self.value == other.value\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "class DecisionTreeBranch:\n",
        "\n",
        "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.low_subtree = low_subtree\n",
        "        self.high_subtree = high_subtree\n",
        "\n",
        "    # For a branch node, we compute the prediction by first considering the feature, and then \n",
        "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
        "    # than the threshold.\n",
        "    def predict(self, x):\n",
        "        if x[self.feature] <= self.threshold:\n",
        "            return self.low_subtree.predict(x)\n",
        "        else:\n",
        "            return self.high_subtree.predict(x)\n",
        "\n",
        "    # Utility function to draw a tree visually using graphviz.\n",
        "    def draw_tree(self, graph, node_counter, names):\n",
        "        node_counter, low_id = self.low_subtree.draw_tree(graph, node_counter, names)\n",
        "        node_counter, high_id = self.high_subtree.draw_tree(graph, node_counter, names)\n",
        "        node_id = str(node_counter)\n",
        "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
        "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
        "        graph.node(node_id, lbl, shape='box', fillcolor='yellow', style='filled, rounded')\n",
        "        graph.edge(node_id, low_id, 'False')\n",
        "        graph.edge(node_id, high_id, 'True')\n",
        "        return node_counter+1, node_id\n",
        "\n",
        "from graphviz import Digraph\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DecisionTree(ABC, BaseEstimator):\n",
        "\n",
        "    def __init__(self, max_depth):\n",
        "        super().__init__()\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
        "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
        "    # called make_tree (see below).\n",
        "    def fit(self, X, Y):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            self.names = X.columns\n",
        "            X = X.to_numpy()\n",
        "        elif isinstance(X, list):\n",
        "            self.names = None\n",
        "            X = np.array(X)\n",
        "        else:\n",
        "            self.names = None\n",
        "        \n",
        "        self.root = self.make_tree(X, Y, self.max_depth)\n",
        "        \n",
        "    def draw_tree(self):\n",
        "        graph = Digraph()\n",
        "        self.root.draw_tree(graph, 0, self.names)\n",
        "        return graph\n",
        "    \n",
        "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
        "    # for a set of instances.\n",
        "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
        "    def predict(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.to_numpy()\n",
        "        return [self.predict_one(x) for x in X]\n",
        "\n",
        "    # Predicting the output for one instance.\n",
        "    def predict_one(self, x):\n",
        "        return self.root.predict(x)        \n",
        "\n",
        "    # This is the recursive training \n",
        "    def make_tree(self, X, Y, max_depth):\n",
        "\n",
        "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
        "        # For classifiers, this will be \n",
        "        default_value = self.get_default_value(Y)\n",
        "\n",
        "        # First the two base cases in the recursion: is the training set completely\n",
        "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
        "\n",
        "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
        "        if max_depth == 0:\n",
        "            return DecisionTreeLeaf(default_value)\n",
        "\n",
        "        # If all the instances in the remaining training set have the same output value,\n",
        "        # return a leaf with this value.\n",
        "        if self.is_homogeneous(Y):\n",
        "            return DecisionTreeLeaf(default_value)\n",
        "\n",
        "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
        "        # we use one of the classification or regression criteria.\n",
        "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
        "        n_features = X.shape[1]\n",
        "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
        "        \n",
        "        if best_feature is None:\n",
        "            return DecisionTreeLeaf(default_value)\n",
        "\n",
        "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
        "        # the threshold or not\n",
        "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
        "\n",
        "        # Build the subtrees using a recursive call. Each subtree is associated\n",
        "        # with a value of the feature.\n",
        "        low_subtree = self.make_tree(X_low, Y_low, max_depth-1)\n",
        "        high_subtree = self.make_tree(X_high, Y_high, max_depth-1)\n",
        "\n",
        "        if low_subtree == high_subtree:\n",
        "            return low_subtree\n",
        "\n",
        "        # Return a decision tree branch containing the result.\n",
        "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
        "    \n",
        "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
        "    # and a threshold.\n",
        "    def split_by_feature(self, X, Y, feature, threshold):\n",
        "        low = X[:,feature] <= threshold\n",
        "        high = ~low\n",
        "        return X[low], X[high], Y[low], Y[high]\n",
        "    \n",
        "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
        "    \n",
        "    @abstractmethod\n",
        "    def get_default_value(self, Y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_homogeneous(self, Y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def best_split(self, X, Y, feature):\n",
        "        pass\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class TreeClassifier(DecisionTree, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, max_depth=10, criterion='maj_sum'):\n",
        "        super().__init__(max_depth)\n",
        "        self.criterion = criterion\n",
        "        \n",
        "    def fit(self, X, Y):\n",
        "        # For decision tree classifiers, there are some different ways to measure\n",
        "        # the homogeneity of subsets.\n",
        "        if self.criterion == 'maj_sum':\n",
        "            self.criterion_function = majority_sum_scorer\n",
        "        elif self.criterion == 'info_gain':\n",
        "            self.criterion_function = info_gain_scorer\n",
        "        elif self.criterion == 'gini':\n",
        "            self.criterion_function = gini_scorer\n",
        "        else:\n",
        "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
        "        super().fit(X, Y)\n",
        "        self.classes_ = sorted(set(Y))\n",
        "\n",
        "    # Select a default value that is going to be used if we decide to make a leaf.\n",
        "    # We will select the most common value.\n",
        "    def get_default_value(self, Y):\n",
        "        self.class_distribution = Counter(Y)\n",
        "        return self.class_distribution.most_common(1)[0][0]\n",
        "    \n",
        "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
        "    # this means that all output values are identical.\n",
        "    # We assume that we called get_default_value just before, so that we can access\n",
        "    # the class_distribution attribute. If the class distribution contains just one item,\n",
        "    # this means that the set is homogeneous.\n",
        "    def is_homogeneous(self, Y):\n",
        "        return len(self.class_distribution) == 1\n",
        "        \n",
        "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
        "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
        "    # In the end, we return a triple consisting of\n",
        "    # - the best score we found, according to the criterion we're using\n",
        "    # - the id of the feature\n",
        "    # - the threshold for the best split\n",
        "    def best_split(self, X, Y, feature):\n",
        "\n",
        "        # Create a list of input-output pairs, where we have sorted\n",
        "        # in ascending order by the input feature we're considering.\n",
        "        XY = sorted(zip(X[:, feature], Y))\n",
        "\n",
        "        n = len(XY)\n",
        "\n",
        "        # The frequency tables corresponding to the parts *before and including*\n",
        "        # and *after* the current element.\n",
        "        low_distr = Counter()\n",
        "        high_distr = Counter(Y)\n",
        "\n",
        "        # Keep track of the best result we've seen so far.\n",
        "        max_score = -np.inf\n",
        "        max_i = None\n",
        "\n",
        "        # Go through all the positions (excluding the last position).\n",
        "        for i in range(0, n-1):\n",
        "\n",
        "            # Input and output at the current position.\n",
        "            x_i = XY[i][0]\n",
        "            y_i = XY[i][1]\n",
        "\n",
        "            # Update the frequency tables.\n",
        "            low_distr[y_i] += 1\n",
        "            high_distr[y_i] -= 1\n",
        "\n",
        "            # If the input is equal to the input at the next position, we will\n",
        "            # not consider a split here.\n",
        "            x_next = XY[i+1][0]\n",
        "            if x_i == x_next:\n",
        "                continue\n",
        "\n",
        "            # Compute the homogeneity criterion for a split at this position.\n",
        "            score = self.criterion_function(i+1, low_distr, n-i-1, high_distr)\n",
        "\n",
        "            # If this is the best split, remember it.\n",
        "            if score > max_score:\n",
        "                max_score = score\n",
        "                max_i = i\n",
        "\n",
        "        # If we didn't find any split (meaning that all inputs are identical), return\n",
        "        # a dummy value.\n",
        "        if max_i is None:\n",
        "            return -np.inf, None, None\n",
        "\n",
        "        # Otherwise, return the best split we found and its score.\n",
        "        split_point = 0.5*(XY[max_i][0] + XY[max_i+1][0])\n",
        "        return score, feature, split_point\n",
        "\n",
        "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
        "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
        "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
        "    return maj_sum_low + maj_sum_high\n",
        "    \n",
        "def entropy(distr):\n",
        "    n = sum(distr.values())\n",
        "    ps = [n_i/n for n_i in distr.values()]\n",
        "    return -sum(p*np.log2(p) if p > 0 else 0 for p in ps)\n",
        "\n",
        "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
        "    return -(n_low*entropy(low_distr)+n_high*entropy(high_distr))/(n_low+n_high)\n",
        "\n",
        "def gini_impurity(distr):\n",
        "    n = sum(distr.values())\n",
        "    ps = [n_i/n for n_i in distr.values()]\n",
        "    return 1-sum(p**2 for p in ps)\n",
        "    \n",
        "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
        "    return -(n_low*gini_impurity(low_distr)+n_high*gini_impurity(high_distr))/(n_low+n_high)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "score=[]\n",
        "N=[]\n",
        "for i in range(2,30):\n",
        "    tc=TreeClassifier(max_depth=i)  \n",
        "    tc.fit(Xtrain, Ytrain)\n",
        "    score.append(cross_val_score(tc, Xtrain, Ytrain, cv=5, scoring='accuracy').mean())\n",
        "    N.append(i)\n",
        "plt.plot(N,score,color='purple',marker='o', markerfacecolor='purple', markersize=4, linewidth=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rZj6rvXE-At"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxj3io6yFk3A"
      },
      "source": [
        "# Task 3: A regression example: predicting apartment prices\n",
        "Here is another dataset. This dataset was created by Sberbank and contains some statistics from the Russian real estate market. Here is the Kaggle page where you can find the original data.\n",
        "\n",
        "Since we will just be able to handle numerical features and not symbolic ones, we'll need with a simplified version of the dataset. So we'll just select 9 of the columns in the dataset. The goal is to predict the price of an apartment, given numerical information such as the number of rooms, the size of the apartment in square meters, the floor, etc. Our approach will be similar to what we did in the classification example: load the data, find a suitable model using cross-validation over the training set, and finally evaluate on the held-out test data.\n",
        "\n",
        "The following code snippet will carry out the basic reading and preprocessing of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11nmg-KQFk3C",
        "outputId": "26e00fda-5235-4e34-c731-30970255d261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "# Read the CSV file using Pandas.\n",
        "alldata = pd.read_csv(https://\"raw.githubusercontent.com/niallomahony93/MLDIoT_Practical1/master/Assignment1/sberbank.csv\")\n",
        "# Convert the timestamp string to an integer representing the year.\n",
        "def get_year(timestamp):\n",
        "    return int(timestamp[:4])\n",
        "alldata['year'] = alldata.timestamp.apply(get_year)\n",
        "\n",
        "# Select the 9 input columns and the output column.\n",
        "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
        "alldata = alldata[selected_columns]\n",
        "alldata = alldata.dropna()\n",
        "\n",
        "# Shuffle.\n",
        "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
        "\n",
        "# Separate the input and output columns.\n",
        "X = alldata_shuffled.drop('price_doc', axis=1)\n",
        "# For the output, we'll use the log of the sales price.\n",
        "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
        "# Split into training and test sets.\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "Y.head(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-95d76d9d7879>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    alldata = pd.read_csv(https://\"raw.githubusercontent.com/niallomahony93/MLDIoT_Practical1/master/Assignment1/sberbank.csv\")\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMSrgJtfFk3K"
      },
      "source": [
        "We train a baseline dummy regressor (which always predicts the same value) and evaluate it in a cross-validation setting.\n",
        "\n",
        "This example looks quite similar to the classification example above. The main differences are (a) that we are predicting numerical values, not symbolic values; (b) that we are evaluating using the mean squared error metric, not the accuracy metric that we used to evaluate the classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzqMdKCfFk3K"
      },
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "m1 = DummyRegressor()\n",
        "cross_validate(m1, Xtrain, Ytrain.astype('int'), scoring='neg_mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzvRE1USFk3O"
      },
      "source": [
        "Replace the dummy regressor with something more meaningful and iterate until you cannot improve the performance. Please note that the cross_validate function returns the negative mean squared error.\n",
        "\n",
        "Some possible regression models that you can try:\n",
        "\n",
        "sklearn.linear_model.LinearRegression\n",
        "\n",
        "sklearn.linear_model.Ridge\n",
        "\n",
        "sklearn.linear_model.Lasso\n",
        "\n",
        "sklearn.tree.DecisionTreeRegressor\n",
        "\n",
        "sklearn.ensemble.RandomForestRegressor\n",
        "\n",
        "sklearn.ensemble.GradientBoostingRegressor\n",
        "\n",
        "sklearn.neural_network.MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoqg5WyuFk3O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE8aWNViFk3b"
      },
      "source": [
        "According to the negative mean squared error, the best results were made using ______ and the error is about ____. \n",
        "\n",
        "Finally, train on the full training set and evaluate on the held-out test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqzOo8dLFk3b"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "  \n",
        "regr.fit(Xtrain, Ytrain)\n",
        "mean_squared_error(Ytest, regr.predict(Xtest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzNsv9zjUjlS"
      },
      "source": [
        "the mean squared error is _____."
      ]
    }
  ]
}