{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of Copy of Practical_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forde1909/MLDIoT_Practical1/blob/master/Copy_of_Copy_of_Practical_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrvM4bz0Fr5F"
      },
      "source": [
        "#Programming assignment 1: introductory tour and decision trees\n",
        "In this assignment, you will take a quick guided tour of the scikit-learn library, one of the most widely used machine learning libraries in Python. We will particularly focus on decision tree learning for classification and regression.\n",
        "\n",
        "\n",
        "There are three tasks of the assignment, where the main focus is on using scikit-learn for training and evaluating machine learning models. \n",
        "\n",
        "Save this notebook in your own google drive/github account, complete your code, plots, and comments and save as a .ipynb file. Name the file as \"Assignment1_your_name.ipynb\" and submit to the assignmet for this practical on blackboard.\n",
        "\n",
        "Deadline: 06-11-2020\n",
        "\n",
        "Didactic purpose of this assignment:\n",
        "\n",
        "*   getting a feel for the workflow of machine learning in Python;\n",
        "*   understanding machine learning algorithms for classification and regression;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5InA2wWFk0n"
      },
      "source": [
        "# Task 1: A classification example: fetal heart condition diagnosis\n",
        "The UCI Machine Learning Repository contains several datasets that can be used to investigate different machine learning algorithms. In this exercise, we'll use a dataset of fetal heart diagnosis. The dataset contains measurements from about 2,600 fetuses. This is a classification task, where our task is to predict a diagnosis type following the FIGO Intrapartum Fetal Monitoring Guidelines: normal, suspicious, or pathological."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUNxdOrFk0s"
      },
      "source": [
        "# Step 1. Reading the data\n",
        "\n",
        "This file contains the data that we will use. This file contains the same data as in the public distribution, except that we converted from Excel to CSV. Download the file and save it in a working directory.\n",
        "\n",
        "Open your favorite editor or a Jupyter notebook. To read the CSV file, it is probably easiest to use the Pandas library. Here is a code snippet that carries out the relevant steps:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ziCVTYBFk0x",
        "outputId": "fa8b9083-a964-41ea-b6ca-253b3e010e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "  \n",
        "# Read the CSV file.\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/niallomahony93/MLDIoT_Practical1/master/Assignment1/CTG.csv\", skiprows=1)\n",
        "\n",
        "# Select the relevant numerical columns.\n",
        "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
        "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
        "                 'Median', 'Variance', 'Tendency', 'NSP']\n",
        "data = data[selected_cols].dropna()\n",
        "\n",
        "# Shuffle the dataset.\n",
        "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
        "\n",
        "# Split into input part X and output part Y.\n",
        "X = data_shuffled.drop('NSP', axis=1)\n",
        "\n",
        "# Map the diagnosis code to a human-readable label.\n",
        "def to_label(y):\n",
        "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n",
        "\n",
        "Y = data_shuffled['NSP'].apply(to_label)\n",
        "\n",
        "\n",
        "# Partition the data into training and test sets.\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "Y.head()\n",
        "X.head()\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LB</th>\n",
              "      <th>AC</th>\n",
              "      <th>FM</th>\n",
              "      <th>UC</th>\n",
              "      <th>DL</th>\n",
              "      <th>DS</th>\n",
              "      <th>DP</th>\n",
              "      <th>ASTV</th>\n",
              "      <th>MSTV</th>\n",
              "      <th>ALTV</th>\n",
              "      <th>MLTV</th>\n",
              "      <th>Width</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>Nmax</th>\n",
              "      <th>Nzeros</th>\n",
              "      <th>Mode</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Median</th>\n",
              "      <th>Variance</th>\n",
              "      <th>Tendency</th>\n",
              "      <th>NSP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>43.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>64.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>132.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>133.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>134.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>132.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.9</td>\n",
              "      <td>117.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      LB   AC   FM   UC   DL  ...   Mean  Median  Variance  Tendency  NSP\n",
              "0  120.0  0.0  0.0  0.0  0.0  ...  137.0   121.0      73.0       1.0  2.0\n",
              "1  132.0  4.0  0.0  4.0  2.0  ...  136.0   140.0      12.0       0.0  1.0\n",
              "2  133.0  2.0  0.0  5.0  2.0  ...  135.0   138.0      13.0       0.0  1.0\n",
              "3  134.0  2.0  0.0  6.0  2.0  ...  134.0   137.0      13.0       1.0  1.0\n",
              "4  132.0  4.0  0.0  5.0  0.0  ...  136.0   138.0      11.0       1.0  1.0\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1kALXYvFk04"
      },
      "source": [
        "# Step 2. Training the baseline classifier\n",
        "\n",
        "We can now start to investigate different classifiers.\n",
        "The DummyClassifier is a simple classifier that does not make use of the features: it just returns the most common label in the training set, in this case Spondylolisthesis. The purpose of using such a stupid classifier is as a baseline: a simple classifier that we can try before we move on to more complex classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EixHbH8mFk07"
      },
      "source": [
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier(strategy='most_frequent')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fidwG14tIfio"
      },
      "source": [
        "To get an idea of how well our simple classifier works, we carry out a cross-validation over the training set and compute the classification accuracy on each fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIAN2Ji8IoGa",
        "outputId": "59cf3f70-9e53-447e-8810-d49fb87f19ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cross_val_score(clf, Xtrain, Ytrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.78235294, 0.78235294, 0.77941176, 0.77941176, 0.77941176])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dCfZ9RPOHD"
      },
      "source": [
        "The result is a NumPy array that contains the accuracies on the different folds in the cross-validation. Get the mean accuracy with the .mean() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsspBBuqJCus",
        "outputId": "3f5d150d-0db8-4ec0-b5d1-5216739d4efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7805882352941176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TjHhnt-Fk1E"
      },
      "source": [
        "## Step 3. Trying out some different classifiers\n",
        "Replace the DummyClassifier with some more meaningful classifier and run the cross-validation again. Try out a few classifiers and see how much you can improve the cross-validation accuracy. Remember, the accuracy is defined as the proportion of correctly classified instances, and we want this value to be high.\n",
        "\n",
        "\n",
        "Here are some possible options:\n",
        "\n",
        "Tree-based classifiers:\n",
        "\n",
        "sklearn.tree.DecisionTreeClassifier\n",
        "\n",
        "sklearn.ensemble.RandomForestClassifier\n",
        "\n",
        "sklearn.ensemble.GradientBoostingClassifier\n",
        "\n",
        "\n",
        "Linear classifiers:\n",
        "\n",
        "sklearn.linear_model.Perceptron\n",
        "\n",
        "sklearn.linear_model.LogisticRegression\n",
        "\n",
        "sklearn.svm.LinearSVC\n",
        "\n",
        "\n",
        "Neural network classifier (will take longer time to train):\n",
        "\n",
        "sklearn.neural_network.MLPClassifier\n",
        "\n",
        "You may also try to tune the hyperparameters of the various classifiers to improve the performance. For instance, the decision tree classifier has a parameter that sets the maximum depth, and in the neural network classifier you can control the number of layers and the number of neurons in each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUDTHf2oFk1F"
      },
      "source": [
        "# The Tree-based classifier using DecisionTreeClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rdH36_yFk1G",
        "outputId": "aa166c63-483d-46e6-cf61-7ceb2c538593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9194117647058823"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVqKHLDJFk1J"
      },
      "source": [
        "# The Tree-based classifier using RandomForestClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Yg75XWFk1M",
        "outputId": "ae617a18-92f7-4598-8936-758552425758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "clf = ensemble.RandomForestClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9411764705882353"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61PWJprlFk1V"
      },
      "source": [
        "# The Tree-based classifier using GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I00Mq5jFk1Z",
        "outputId": "02531871-2895-45b4-aa5b-fc70d24db4d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "clf = ensemble.GradientBoostingClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9488235294117647"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStwsBJYFk1k"
      },
      "source": [
        "# Linear classifier using Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtFidw3CFk1l",
        "outputId": "c09d6358-0b3b-47cd-8329-013e5cfba875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Perceptron()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.825294117647059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je04C9W5Fk1p"
      },
      "source": [
        "# Linear classifier using LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK8zzfkAFk1p",
        "outputId": "6a43c9f2-6483-4965-8039-3b8f025d3f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.LogisticRegression()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8735294117647058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSqJ2KUPFk10"
      },
      "source": [
        "# Linear classifier using LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5LFSpADFk12",
        "outputId": "6932c005-81b7-4455-b352-e7d0ffb5ec1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.836470588235294"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQmRypeP6lo"
      },
      "source": [
        " Neural Network Using Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcOMM3NOFk17",
        "outputId": "d7291043-3fbc-4563-c3c4-4d36c4a6a320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import neural_network\n",
        "clf = neural_network.MLPClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8800000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8z-1HaOFk2G"
      },
      "source": [
        "# Step 4. Final evaluation\n",
        "When you have found a classifier that gives a high accuracy in the cross-validation evaluation, train it on the whole training set and evaluate it on the held-out test set. Please include a description of the classifier you selected and report its accuracy below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyT5Yqs5Fk2G"
      },
      "source": [
        "According to step 3, the best results were made using GradientBoostingClassifier and the accuracy is about 0.9488235294117647."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk0fh09UFk2H",
        "outputId": "53d31de7-e17c-4f1d-99f7-5f9632fe6a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "  \n",
        "clf.fit(Xtrain, Ytrain)\n",
        "Yguess = clf.predict(Xtest)\n",
        "print(accuracy_score(Ytest, Yguess))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9272300469483568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQnMHsLSFk2L"
      },
      "source": [
        "# Task 2: Decision trees for classification\n",
        "Import the code from Lecture1.ipynb and use the defined class TreeClassifier as your classifier in an experiment similar to those in Task 1. Tune the hyperparameter max_depth to get the best cross-validation performance, and then evaluate the classifier on the test set.\n",
        "\n",
        "Please report below what value of max_depth you selected and what accuracy you got.\n",
        "\n",
        "For illustration, let's also draw a tree. Set max_depth to a reasonable small value, and then call draw_tree to visualize the learned decision tree. Include this tree in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ssyp1b3Fk26"
      },
      "source": [
        "The accuracy of the classifier in the test set is ____. The accuracy is largely increased at the max_depth _______. The learned decision tree is visualized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiC2IwGhFk27",
        "outputId": "ae9b6379-e081-4683-afb4-1d614b616233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "clf = ensemble.RandomForestClassifier()\n",
        "clf=clf.fit(Xtrain, Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9394117647058824"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_48wtXTE9T-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rZj6rvXE-At"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxj3io6yFk3A"
      },
      "source": [
        "# Task 3: A regression example: predicting apartment prices\n",
        "Here is another dataset. This dataset was created by Sberbank and contains some statistics from the Russian real estate market. Here is the Kaggle page where you can find the original data.\n",
        "\n",
        "Since we will just be able to handle numerical features and not symbolic ones, we'll need with a simplified version of the dataset. So we'll just select 9 of the columns in the dataset. The goal is to predict the price of an apartment, given numerical information such as the number of rooms, the size of the apartment in square meters, the floor, etc. Our approach will be similar to what we did in the classification example: load the data, find a suitable model using cross-validation over the training set, and finally evaluate on the held-out test data.\n",
        "\n",
        "The following code snippet will carry out the basic reading and preprocessing of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11nmg-KQFk3C",
        "outputId": "cc254fe0-35f0-4d35-c7b8-f16a21ba500d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "# Read the CSV file using Pandas.\n",
        "alldata = pd.read_csv('sberbank.csv')\n",
        "\n",
        "# Convert the timestamp string to an integer representing the year.\n",
        "def get_year(timestamp):\n",
        "    return int(timestamp[:4])\n",
        "alldata['year'] = alldata.timestamp.apply(get_year)\n",
        "\n",
        "# Select the 9 input columns and the output column.\n",
        "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
        "alldata = alldata[selected_columns]\n",
        "alldata = alldata.dropna()\n",
        "\n",
        "# Shuffle.\n",
        "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
        "\n",
        "# Separate the input and output columns.\n",
        "X = alldata_shuffled.drop('price_doc', axis=1)\n",
        "# For the output, we'll use the log of the sales price.\n",
        "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
        "# Split into training and test sets.\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "Y.head(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-64571e10b78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read the CSV file using Pandas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sberbank.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Convert the timestamp string to an integer representing the year.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sberbank.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMSrgJtfFk3K"
      },
      "source": [
        "We train a baseline dummy regressor (which always predicts the same value) and evaluate it in a cross-validation setting.\n",
        "\n",
        "This example looks quite similar to the classification example above. The main differences are (a) that we are predicting numerical values, not symbolic values; (b) that we are evaluating using the mean squared error metric, not the accuracy metric that we used to evaluate the classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzqMdKCfFk3K"
      },
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "m1 = DummyRegressor()\n",
        "cross_validate(m1, Xtrain, Ytrain.astype('int'), scoring='neg_mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzvRE1USFk3O"
      },
      "source": [
        "Replace the dummy regressor with something more meaningful and iterate until you cannot improve the performance. Please note that the cross_validate function returns the negative mean squared error.\n",
        "\n",
        "Some possible regression models that you can try:\n",
        "\n",
        "sklearn.linear_model.LinearRegression\n",
        "\n",
        "sklearn.linear_model.Ridge\n",
        "\n",
        "sklearn.linear_model.Lasso\n",
        "\n",
        "sklearn.tree.DecisionTreeRegressor\n",
        "\n",
        "sklearn.ensemble.RandomForestRegressor\n",
        "\n",
        "sklearn.ensemble.GradientBoostingRegressor\n",
        "\n",
        "sklearn.neural_network.MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoqg5WyuFk3O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE8aWNViFk3b"
      },
      "source": [
        "According to the negative mean squared error, the best results were made using ______ and the error is about ____. \n",
        "\n",
        "Finally, train on the full training set and evaluate on the held-out test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqzOo8dLFk3b"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "  \n",
        "regr.fit(Xtrain, Ytrain)\n",
        "mean_squared_error(Ytest, regr.predict(Xtest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzNsv9zjUjlS"
      },
      "source": [
        "the mean squared error is _____."
      ]
    }
  ]
}